# Head-Pose-Estimation
#                                                           Hi ðŸ‘‹, I'm Zyad Samy
In this project, I trained multible machine learning models using 2D landmarks and the AFLW2000 dataset which consists of 2000 images annotated with image-level 68-point 3D facial landmarks in a variety of head poses and Mediapipe library that extract 468 points ( X, Y) to estimate the rotation axis (pitch,yaw,roll) for the head in real time
### Differance between pitch, yaw and roll:
![image](https://github.com/ZyadSamy96/Head-Pose-Estimation/assets/94635686/61b2f622-160f-491f-85b9-d190cf15de86)

## Final Output
![out (45)](https://user-images.githubusercontent.com/94635686/221700347-5efe4c54-d0a2-49d6-aba4-a614a16f03e4.gif)

- MediaPipe FaceMesh contains a bounding box of the detected face and an array of 468 keypoints. Each keypoint or facial landmark has 2D (x, y).
- You can check it throch this [LINK](https://i.stack.imgur.com/5Mohl.jpg)

## Used Dependencies 
- numpy
- os
- cv2
- glob
- random
- scipy.io
- Math 
- Pathlib
- Pandas 
- Mediapipe
- Sklearn





# ðŸ”— Links
[![image](https://user-images.githubusercontent.com/94635686/221719442-0f295fc5-a135-44e6-b15f-90dbd787086b.png)](https://www.linkedin.com/in/zyad-samy-b2b4b4191/)


